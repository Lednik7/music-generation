{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install transformers\n!pip install pretty_midi\n!pip install gdown\n!pip install music21","metadata":{"id":"TKlhvGP37Nak","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPTNeoForCausalLM\nimport torch\nimport os\nimport pretty_midi\nfrom tqdm.notebook import tqdm\nfrom random import randint\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold","metadata":{"id":"mDCmwCl07xeq","execution":{"iopub.status.busy":"2022-08-12T08:21:38.507765Z","iopub.execute_input":"2022-08-12T08:21:38.508138Z","iopub.status.idle":"2022-08-12T08:21:39.688356Z","shell.execute_reply.started":"2022-08-12T08:21:38.508107Z","shell.execute_reply":"2022-08-12T08:21:39.687100Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"RANGE_NOTE_ON = 128\nRANGE_NOTE_OFF = 128\nRANGE_VEL = 32\nRANGE_TIME_SHIFT = 100\n\nTOKEN_END = RANGE_NOTE_ON + RANGE_NOTE_OFF + RANGE_VEL + RANGE_TIME_SHIFT\nTOKEN_PAD = TOKEN_END + 1\nTOKEN_START = TOKEN_END + 1\nVOCAB_SIZE = TOKEN_PAD + 1 + 4 + 1","metadata":{"id":"OttlfiyDEpzP","execution":{"iopub.status.busy":"2022-08-12T08:21:39.690874Z","iopub.execute_input":"2022-08-12T08:21:39.691834Z","iopub.status.idle":"2022-08-12T08:21:39.698146Z","shell.execute_reply.started":"2022-08-12T08:21:39.691793Z","shell.execute_reply":"2022-08-12T08:21:39.697133Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nNOTE_ON = 0\nNOTE_OFF = 1\nSET_VELOCITY = 2\nTIME_SHIFT = 3\n\nMAX_TIME_SHIFT = 1.0\nTIME_SHIFT_STEP = 0.01\nRANGES = [128,128,32,100]\n\nPIANO_RANGE = [21,96]  # 76 piano keys\n\n\ndef encode(midi, use_piano_range=True):\n    \"\"\"\n    Encodes midi to event-based sequences for MusicTransformer.\n    \n    Parameters\n    ----------\n    midi : prettyMIDI object\n        MIDI to encode.\n    use_piano_range : bool\n        if True, classical piano range will be used for skip pitches. Pitches which are not in range PIANO_RANGE will be skipped.\n    \n    Returns\n    -------\n    encoded_splits : list(list())\n        splits of encoded sequences.\n    \"\"\"\n    events = get_events(midi, use_piano_range=use_piano_range)\n    if len(events) == 0:\n        return []\n    quantize_(events)\n    add_time_shifts(events)\n    encoded = encode_events(events)\n    return encoded\n    \n    \ndef decode(encoded):\n    \"\"\"\n    Decode event-based encoded sequence into MIDI object.\n    \n    Parameters\n    ----------\n    encoded : np.array or list\n        encoded sequence to decode.\n    \n    Returns\n    -------\n    midi_out: PrettyMIDI object\n        decoded MIDI.\n    \"\"\"\n    midi_out = pretty_midi.PrettyMIDI()\n    midi_out.instruments.append(pretty_midi.Instrument(0, name='piano'))\n    notes = midi_out.instruments[0].notes\n    \n    notes_tmp = {}  # pitch: [vel, start, end]\n    cur_time = 0\n    cur_velocity = 100\n    for ev in encoded:\n        if ev < RANGES[0]:\n            # NOTE_ON\n            pitch = ev\n            if notes_tmp.get(pitch) is None:\n                notes_tmp[pitch] = [cur_velocity, cur_time]\n        elif ev >= RANGES[0] and ev < sum(RANGES[:2]):\n            # NOTE_OFF\n            pitch = ev - RANGES[0]\n            note = notes_tmp.get(pitch)\n            if note is not None:  # check for overlaps (first-OFF mode)\n                notes.append(pretty_midi.Note(note[0], pitch, note[1], cur_time))\n                notes_tmp.pop(pitch)\n        elif ev >= sum(RANGES[:2]) and ev < sum(RANGES[:3]):\n            # SET_VELOCITY\n            cur_velocity = max(1,(ev - sum(RANGES[:2]))*128//RANGES[2])\n        elif ev >= sum(RANGES[:3]) and ev < sum(RANGES[:]):\n            # TIME_SHIFT\n            cur_time += (ev - sum(RANGES[:3]) + 1)*TIME_SHIFT_STEP\n        else:\n            continue\n\n    for pitch, note in notes_tmp.items():\n        if note[1] != cur_time:\n            notes.append(pretty_midi.Note(note[0], pitch, note[1], cur_time))\n        \n    return midi_out\n\n\ndef round_step(x, step=0.01):\n    return round(x/step)*step\n\n\ndef get_events(midi, use_piano_range=False):\n    # helper function used in encode()\n    # time, type, value\n    events = []\n    for inst in midi.instruments:\n        if inst.is_drum:\n            continue\n        for note in inst.notes:\n            if use_piano_range and not (PIANO_RANGE[0] <= note.pitch <= PIANO_RANGE[1]):\n                continue\n            start = note.start\n            end = note.end\n            events.append([start, SET_VELOCITY, note.velocity])\n            events.append([start, NOTE_ON, note.pitch])\n            events.append([end, NOTE_OFF, note.pitch])\n    events = sorted(events, key=lambda x: x[0])\n    return events\n\n\ndef quantize_(events):\n    for ev in events:\n        ev[0] = round_step(ev[0])\n\n\ndef add_time_shifts(events):\n    # populate time_shifts, helper function used in encode()\n    times = np.array(list(zip(*events)))[0]\n    diff = np.diff(times, prepend=0)\n    idxs = diff.nonzero()[0]\n    for i in reversed(idxs):\n        if i == 0:\n            continue\n        t0 = events[i-1][0] # if i != 0 else 0\n        t1 = events[i][0]\n        dt = t1-t0\n        events.insert(i, [t0, TIME_SHIFT, dt])\n\n\ndef encode_events(events):\n    # helper function used in encode()\n    out = []\n    types = []\n    for time, typ, value in events:\n        offset = sum(RANGES[:typ])\n\n        if typ == SET_VELOCITY:\n            value = value*RANGES[SET_VELOCITY]//128\n            out.append(offset+value)\n            types.append(typ)\n\n        elif typ == TIME_SHIFT:\n            dt = value\n            n = RANGES[TIME_SHIFT]\n            enc = lambda x: int(x*n)-1\n            for _ in range(int(dt//MAX_TIME_SHIFT)):\n                out.append(offset+enc(MAX_TIME_SHIFT))\n                types.append(typ)\n            r = round_step(dt%MAX_TIME_SHIFT, TIME_SHIFT_STEP)\n            if r > 0:\n                out.append(offset+enc(r))\n                types.append(typ)\n\n        else:\n            out.append(offset+value)\n            types.append(typ)\n            \n    return out\n\n\nRANGES_SUM = np.cumsum(RANGES)\n\n\ndef get_type(ev):\n    if ev < RANGES_SUM[0]:\n        # NOTE_ON\n        return 0\n    elif ev < RANGES_SUM[1]:\n        # NOTE_OFF\n        return 1\n    elif ev < RANGES_SUM[2]:\n        # VEL\n        return 2\n    elif ev < RANGES_SUM[3]:\n        # TS\n        return 3\n    else:\n        return -1\n\n    \ndef filter_bad_note_offs(events):\n    \"\"\"Clear NOTE_OFF events for which the corresponding NOTE_ON event is missing.\"\"\"\n    notes_down = {}  # pitch: 1\n    keep_idxs = set(range(len(events)))\n\n    for i,ev in enumerate(events):\n        typ = get_type(ev)\n\n        if typ == NOTE_ON:\n            pitch = ev\n            notes_down[pitch] = 1\n        if typ == NOTE_OFF:\n            pitch = ev-128\n            if notes_down.get(pitch) is None:\n                # if NOTE_OFF without NOTE_ON, then remove the event\n                keep_idxs.remove(i)\n            else:\n                notes_down.pop(pitch)\n    \n    return list(keep_idxs)","metadata":{"id":"xq54LUm89stl","execution":{"iopub.status.busy":"2022-08-12T08:21:39.713953Z","iopub.execute_input":"2022-08-12T08:21:39.714450Z","iopub.status.idle":"2022-08-12T08:21:39.743338Z","shell.execute_reply.started":"2022-08-12T08:21:39.714412Z","shell.execute_reply":"2022-08-12T08:21:39.742265Z"},"_kg_hide-input":false,"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom tqdm import tqdm\n\n\n\ndef generate(model, primer, target_seq_length=1024, temperature=1.0, topk=40, topp=0.99, topp_temperature=1.0, at_least_k=1, use_rp=False, rp_penalty=0.05, rp_restore_speed=0.7, seed=None, **forward_args):\n    \"\"\"\n    Generate batch of samples, conditioned on `primer`. There are used several techniques for acquiring better generated samples such as:\n    - temperature skewing for controlling entropy of distribuitions\n    - top-k sampling\n    - top-p (nucleus) sampling (https://arxiv.org/abs/1904.09751)\n    - DynamicRepetitionPenaltyProcessor that prevents notes repeating\n    values by default usualy are suitable for our models\n        \n    Parameters\n    ----------\n    model : MusicTransformer\n        trained model.\n    primer : torch.Tensor (B x N)\n        primer for condition on.\n        B = batch_size, N = seq_lenght.\n        We are using the primer consisted of one token - genre. These tokens are {390:'classic', 391:'jazz', 392:'calm', 393:'pop'}.\n    target_seq_length : int\n        desired length  of generated sequences.\n    temperature : float\n        temperature alters the output distribuition of the model. Higher values ( > 1.0) lead to more stohastic sampling, lower values lead to more expected and predictable sequences (ending up with endlessly repeating musical patterns).\n    topk : int\n        restricts sampling from lower probabilities. It is the length of set of tokens from which sampling will be.\n    topp : float\n        restricts sampling from lower probabilities, but more adaptive then topk. see (https://arxiv.org/abs/1904.09751).\n    topp_temperature : float\n        temperature for counting cumulative sum doing topp sampling.\n    at_least_k : int\n        like topk, but force to sample from at least k tokens of higher probabilities.\n    use_rp : bool\n        use or not the DynamicRepetitionPenaltyProcessor (RP). Trying to prevent the generation of repeated notes.\n    rp_penalty : float\n        coef for RP. Higher values lead to more RP impact.\n    rp_restore_speed : float\n        how fast the penalty will be lifted. Lower values lead to more RP impact.\n    seed : int\n        fixes seed for deterministic generation.\n    forward_args : dict\n        args for model's forward.\n        \n    Returns\n    -------\n    generated : torch.Tensor (B x target_seq_length)\n        generated batch of sequences.\n    \"\"\"\n    device = model.device\n    if seed is not None:\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n    \n    if at_least_k < 1:\n        at_least_k = 1\n    B,N = primer.shape\n    generated = torch.full((B,target_seq_length), TOKEN_PAD, dtype=torch.int64, device=device)\n    generated[..., :N] = primer.to(device)\n    \n    if use_rp:\n        RP_processor = DynamicRepetitionPenaltyProcessor(B, penalty=rp_penalty, restore_speed=rp_restore_speed, device=device)\n    whitelist_mask = make_whitelist_mask()\n    \n    model.eval()\n    with torch.no_grad():\n        for i in tqdm(range(N, target_seq_length)):\n            logits = model(generated[:, :i], **forward_args)[:, i-1, :]\n            logits[:,~whitelist_mask] = float('-inf')\n            p = torch.softmax(logits/topp_temperature, -1)\n            \n            # apply topk:\n            if topk == 0:\n                topk = p.shape[-1]\n            p_topk, idxs = torch.topk(p, topk, -1, sorted=True)\n            \n            # apply topp:\n            mask = p_topk.cumsum(-1) < topp\n            mask[:,:at_least_k] = True\n            logits_masked = logits.gather(-1, idxs)\n            logits_masked[~mask] = float('-inf')\n            p_topp = torch.softmax(logits_masked/temperature, -1)\n            \n            # apply penalty:\n            if use_rp:\n                p_penalized = RP_processor.apply_penalty(p_topp, idxs)\n                ib = p_penalized.sum(-1) == 0\n                if ib.sum() > 0:\n                    # if all topp tokens get zeroes due RP_processor, then fallback to topk-sampling\n                    p_fallback = p_topk[ib].clone()\n                    p_fallback[mask[ib]] = 0.  # zeroing topp\n                    p_penalized[ib] = p_fallback\n                    \n                ib = p_penalized.sum(-1) == 0\n                if ib.sum() > 0:\n                    # if topk tokens get zeroes, fallback to topp without RP\n                    print('fallback-2')\n                    p_penalized = p_topp\n                p_topp = p_penalized\n                    \n            # sample:\n            next_token = idxs.gather(-1, torch.multinomial(p_topp, 1))\n            generated[:, i] = next_token.squeeze(-1)\n            \n            # update penalty:\n            if use_rp:\n                RP_processor.update(next_token)\n\n    return generated[:, :i+1]\n\n\ndef post_process(generated, remove_bad_generations=True):\n    \"\"\"\n    Post-process does 3 routines:\n        1) removes long pauses (3+ seconds)\n        2) clips velocities to range(30,100) to avoid dramaticly loud notes, which are not suitable for our case.\n        3) removes bad generated samples. The model sometimes may generate music that consists only of many repeating notes. We try to detect them and remove from batch.\n        \n    Parameters\n    ----------\n    generated : torch.Tensor (B x N)\n        batch of generated samples\n        \n    Returns\n    -------\n    filtered_generated : cleaner and slightly better sounding generated batch\n    \"\"\"\n    generated = generated.cpu().numpy()\n    remove_pauses(generated, 3)\n    clip_velocity(generated)\n    \n    bad_filter = np.ones(len(generated), dtype=bool)\n    \n    if remove_bad_generations:\n        for i, gen in enumerate(generated):\n            midi = decode(gen)\n            if detect_note_repetition(midi) > 0.9:\n                bad_filter[i] = False\n\n        if np.sum(bad_filter) != len(bad_filter):\n            print(f'{np.sum(~bad_filter)} bad samples will be removed.')\n        \n    return generated[bad_filter]\n    \n\ndef make_whitelist_mask():\n    \"\"\"Generate mask for PIANO_RANGE\"\"\"\n    whitelist_mask = np.zeros(VOCAB_SIZE, dtype=bool)\n    whitelist_mask[PIANO_RANGE[0]:PIANO_RANGE[1]+1] = True\n    whitelist_mask[128+PIANO_RANGE[0]:128+PIANO_RANGE[1]+1] = True\n    whitelist_mask[128*2:] = True\n    return whitelist_mask\n\n    \nclass DynamicRepetitionPenaltyProcessor:\n    \"\"\"\n    The class is trying to prevent cases where the model generates repetitive notes or musical patterns that degrade quality.\n    It dynamically reduces and restores the probabilities of generatied notes.\n    Each generated note will reduce its probability for the next step by `penalty` value (which is hyperparameter). If this note has been generated again, then we continue to reduce its probability, else we will gradually restore its probability (speed is controlled by restore_speed parameter).\n    \n    Parameters\n    ----------\n    bs : int\n        batch_size. We need to know batch_size in advance to create the penalty_matrix.\n    penalty : float\n        value by which the probability will be reduced.\n    restore_speed : float\n        the number inversed to the number of seconds needs to fully restore probability from 0 to 1.\n        for restore_speed equal to 1.0 we need 1.0 sec to restore, for 2.0 - 0.5 sec and so on.\n    \"\"\"\n    def __init__(self, bs, device, penalty=0.3, restore_speed=1.0) :\n        self.bs = bs\n        self.penalty = penalty\n        self.restore_speed = restore_speed\n        self.penalty_matrix = torch.ones(bs,128).to(device)\n        \n    def apply_penalty(self, p, idxs):\n        p = p.clone()\n        for b in range(len(p)):\n            i = idxs[b]\n            pi = p[b]\n            mask = i < 128\n            if len(i) > 0:\n                pi[mask] = pi[mask]*self.penalty_matrix[b,i[mask]]\n        return p\n        \n    def update(self, next_token):\n        restoring = next_token - (128+128+32)  # only TS do restore\n        restoring = torch.clamp(restoring.float(), 0, 100)/100*self.restore_speed\n        self.penalty_matrix += restoring\n        nt = next_token.squeeze(-1)\n        nt = next_token[next_token < 128]\n        self.penalty_matrix[:, nt] -= restoring + self.penalty\n        torch.clamp(self.penalty_matrix, 0, 1.0, out=self.penalty_matrix)\n        return restoring, nt\n    \n\ndef detect_note_repetition(midi, threshold_sec=0.01):\n    \"\"\"\n    Returns the fraction of note repetitions. Counts cases where prev_note_end == next_note_start at the same pitch ('glued' notes). Used in detection bad generated samples.\n    \n    Parameters\n    ----------\n    midi : prettyMIDI object\n    threshold_sec : float\n        intervals smaller then threshold_sec are treated as 'glued' notes.\n    \n    Returns\n    -------\n    fraction of notes repetitions relative to the number of all notes.\n    \"\"\"\n    all_notes = [x for inst in midi.instruments for x in inst.notes if not inst.is_drum]\n    if len(all_notes) == 0:\n        return 0\n    all_notes_np = np.array([[x.start,x.end,x.pitch,x.velocity] for x in all_notes])\n    \n    i_sort = np.lexsort([all_notes_np[:,0], all_notes_np[:,2]])\n\n    s = []\n    cur_p = -1\n    cur_t = -1\n    for t in all_notes_np[i_sort]:\n        a,b,p,v = t\n        if cur_p != p:\n            cur_p = p\n        else:\n            s.append(a-cur_t)\n        cur_t = b\n    s = np.array(s)\n    return (s < threshold_sec).sum()/len(s)\n\n\ndef remove_pauses(generated, threshold=3):\n    \"\"\"\n    Fills  pauses by constants.TOKEN_PAD values. Only pauses that longer than `threshold` seconds are considered.\n    Inplace operation. `generated` is a tensor (batch of sequences).\n    \n    Parameters\n    ----------\n    generated : torch.Tensor (B x N)\n        generated batch of sequences.\n    threshold : int/float\n        the minimum seconds of silence to treat them as a pause.\n    \"\"\"\n    mask = (generated>=RANGES_SUM[2]) & (generated<RANGES_SUM[3])\n    seconds = ((generated-RANGES_SUM[2])+1)*0.01\n    seconds[~mask] = 0\n\n    res_ab = [[] for _ in range(seconds.shape[0])]\n\n    for ib,i_seconds in enumerate(seconds):\n        a,s = 0,0\n        notes_down = np.zeros(128, dtype=bool)\n        for i,(t,ev) in enumerate(zip(i_seconds,generated[ib])):\n            typ = get_type(ev)\n            if typ == NOTE_ON:\n                pitch = ev\n                notes_down[pitch] = True\n            if typ == NOTE_OFF:\n                pitch = ev-128\n                notes_down[pitch] = False\n                    \n            if t == 0:\n                if s >= threshold and notes_down.sum() == 0:\n                    res_ab[ib].append([a,i,s])\n                s = 0\n                a = i+1\n            s += t\n        if s >= threshold and notes_down.sum() == 0:\n            res_ab[ib].append([a,len(i_seconds),s])\n    \n    # remove inplace\n    for ib,t in enumerate(res_ab):\n        for a,b,s in t:\n            generated[ib, a:b] = TOKEN_PAD\n            print(f'pause removed:',ib,f'n={b-a}',a,b,s)\n\n        \ndef clip_velocity(generated, min_velocity=30, max_velocity=100):\n    \"\"\"\n    Clip velocity to range(min_velocity, max_velocity). Since the model sometimes generate overloud sequences, we try to neutralize this effect.\n    Inplace operation. `generated` is a tensor (batch of sequences).\n    \n    Parameters\n    ----------\n    generated : torch.Tensor (B x N)\n        generated batch of sequences.\n    min_velocity : int\n    max_velocity : int\n    \"\"\"\n    max_velocity_encoded = max_velocity*32//128 + RANGES_SUM[1]\n    min_velocity_encoded = min_velocity*32//128 + RANGES_SUM[1]\n    \n    mask = (generated>=RANGES_SUM[1]) & (generated<RANGES_SUM[2])\n    generated[mask] = np.clip(generated[mask], min_velocity_encoded, max_velocity_encoded)","metadata":{"id":"unxlLEj5QP5z","execution":{"iopub.status.busy":"2022-08-12T08:21:39.745421Z","iopub.execute_input":"2022-08-12T08:21:39.746241Z","iopub.status.idle":"2022-08-12T08:21:39.784181Z","shell.execute_reply.started":"2022-08-12T08:21:39.746201Z","shell.execute_reply":"2022-08-12T08:21:39.782968Z"},"_kg_hide-input":false,"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def gen_batch(inputs, batch_size):\n    batch_start = 0\n    while batch_start < len(inputs):\n        yield inputs[batch_start: batch_start + batch_size]\n        batch_start += batch_size\n\nclass MidiDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        dir,\n        max_len=128\n    ):  \n        self.paths = [os.path.join(dir, i) for i in os.listdir(dir) if i[-4:] == '.mid'][:200]\n        self.max_len = max_len\n        self.dataset = []\n        \n        for path in tqdm(self.paths):\n            try:\n                midi_file = pretty_midi.PrettyMIDI(path)\n                notes = encode(midi_file, use_piano_range=False)\n                for batch in gen_batch(notes, self.max_len - 2):\n                    notes = [TOKEN_START] + batch + [TOKEN_END]\n                    attention_mask = list(torch.ones(len(notes)))\n                    if len(notes) < self.max_len:\n                        attention_mask += [0] * (self.max_len - len(notes))\n                        notes += [TOKEN_PAD] * (self.max_len - len(notes))\n                    self.dataset.append([torch.tensor(notes).long(), torch.tensor(attention_mask).long()])\n            except: pass\n            \n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self,idx):\n        return self.dataset[idx]","metadata":{"id":"sK1ih6sD9xvF","execution":{"iopub.status.busy":"2022-08-12T08:21:39.788016Z","iopub.execute_input":"2022-08-12T08:21:39.790315Z","iopub.status.idle":"2022-08-12T08:21:39.800436Z","shell.execute_reply.started":"2022-08-12T08:21:39.790285Z","shell.execute_reply":"2022-08-12T08:21:39.799550Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"!rm -rf music && mkdir music\n!cd music && gdown --fuzzy https://drive.google.com/file/d/13gDAcndQufUD27dNZg-OhxNk5-vgxtan/view?usp=sharing \\\n    && unzip midi_post_train.zip && rm -rf midi_post_train.zip","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:21:58.486039Z","iopub.execute_input":"2022-08-12T08:21:58.487522Z","iopub.status.idle":"2022-08-12T08:22:01.893521Z","shell.execute_reply.started":"2022-08-12T08:21:58.487438Z","shell.execute_reply":"2022-08-12T08:22:01.892360Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"!gdown --fuzzy https://drive.google.com/file/d/1ZLhYZObENV1_oS_CPqrPZrThwrWH84ju/view?usp=sharing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -u music_all.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = MidiDataset('music', 512)","metadata":{"id":"PYNBXkvyDtgv","execution":{"iopub.status.busy":"2022-08-12T08:22:02.642983Z","iopub.execute_input":"2022-08-12T08:22:02.643430Z","iopub.status.idle":"2022-08-12T08:22:06.148570Z","shell.execute_reply.started":"2022-08-12T08:22:02.643388Z","shell.execute_reply":"2022-08-12T08:22:06.147553Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size=8,\n            shuffle=True,\n            num_workers=2,\n        )","metadata":{"id":"22_2HbKbBRpE","execution":{"iopub.status.busy":"2022-08-12T08:22:07.010573Z","iopub.execute_input":"2022-08-12T08:22:07.010932Z","iopub.status.idle":"2022-08-12T08:22:07.016113Z","shell.execute_reply.started":"2022-08-12T08:22:07.010900Z","shell.execute_reply":"2022-08-12T08:22:07.014908Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-125M')","metadata":{"id":"K6o92IkdFf_s","execution":{"iopub.status.busy":"2022-08-12T08:22:10.986451Z","iopub.execute_input":"2022-08-12T08:22:10.987424Z","iopub.status.idle":"2022-08-12T08:22:13.869094Z","shell.execute_reply.started":"2022-08-12T08:22:10.987389Z","shell.execute_reply":"2022-08-12T08:22:13.868036Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"../input/midineo/gpt_neo_all_genres_ep7\")[\"model\"])","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:23:19.193125Z","iopub.execute_input":"2022-08-12T08:23:19.193527Z","iopub.status.idle":"2022-08-12T08:23:20.469279Z","shell.execute_reply.started":"2022-08-12T08:23:19.193470Z","shell.execute_reply":"2022-08-12T08:23:20.468154Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"#model.lm_head = torch.nn.Linear(768, VOCAB_SIZE, bias=False)\nmodel = model.to('cuda')","metadata":{"id":"dYCmhDTUHLXy","outputId":"79dae5d4-17f4-475d-93a2-53e9ebe17490","execution":{"iopub.status.busy":"2022-08-12T08:23:22.759700Z","iopub.execute_input":"2022-08-12T08:23:22.760286Z","iopub.status.idle":"2022-08-12T08:23:22.772586Z","shell.execute_reply.started":"2022-08-12T08:23:22.760251Z","shell.execute_reply":"2022-08-12T08:23:22.771321Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"epochs = 3","metadata":{"id":"3hnlehqCGbaV","execution":{"iopub.status.busy":"2022-08-12T08:23:25.438996Z","iopub.execute_input":"2022-08-12T08:23:25.439766Z","iopub.status.idle":"2022-08-12T08:23:25.444731Z","shell.execute_reply.started":"2022-08-12T08:23:25.439722Z","shell.execute_reply":"2022-08-12T08:23:25.443690Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(),\n                  lr = 2e-5,\n                  eps = 1e-8\n                )","metadata":{"id":"93xy1jDBFuBM","execution":{"iopub.status.busy":"2022-08-12T08:23:28.003332Z","iopub.execute_input":"2022-08-12T08:23:28.004321Z","iopub.status.idle":"2022-08-12T08:23:28.010765Z","shell.execute_reply.started":"2022-08-12T08:23:28.004262Z","shell.execute_reply":"2022-08-12T08:23:28.009666Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, total_steps=len(train_loader) * epochs, max_lr=1e-4, div_factor=25, pct_start=0.1)","metadata":{"id":"UZXqrVL7GV9v","execution":{"iopub.status.busy":"2022-08-12T08:23:28.181672Z","iopub.execute_input":"2022-08-12T08:23:28.182781Z","iopub.status.idle":"2022-08-12T08:23:28.188399Z","shell.execute_reply.started":"2022-08-12T08:23:28.182734Z","shell.execute_reply":"2022-08-12T08:23:28.187271Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def save_model(name, model, epoch=None, train_loss=None, val_loss=None, optimizer=None, scheduler=None):\n    '''Save PyTorch model.'''\n\n    torch.save({\n        'model': model.state_dict(),\n        'epoch': epoch,\n        'train_loss': train_loss,\n#         'val_loss': val_loss,\n        'optimizer': optimizer.state_dict(),\n        'scheduler': scheduler.state_dict(),\n    }, os.path.join('./', name))","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:23:28.348322Z","iopub.execute_input":"2022-08-12T08:23:28.349066Z","iopub.status.idle":"2022-08-12T08:23:28.355506Z","shell.execute_reply.started":"2022-08-12T08:23:28.349027Z","shell.execute_reply":"2022-08-12T08:23:28.354300Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\naverage_loss = []\nfor epoch in tqdm(range(epochs)):\n    average_loss = []\n    for i, batch in enumerate(tqdm(train_loader)):\n        optimizer.zero_grad() \n        input_ids, attention_mask = batch[0].to('cuda'), batch[1].to('cuda')       \n        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids.clone())['loss']\n        loss.backward()\n        average_loss.append(loss.item())\n        optimizer.step()\n        scheduler.step()\n        print('step', i, 'loss', loss.item(), 'mean_loss', np.array(average_loss).mean())#step 600 loss 2.349747657775879 mean_loss 2.5898751151561736\n            \n    save_model(f'gpt_neo_all_genres_ep{6 + epoch}', model=model, epoch=epoch, optimizer=optimizer, scheduler=scheduler)\n    average_loss = []","metadata":{"id":"G9-EjH6TGaMC","outputId":"4ed7ccb1-7f0f-41c8-e07c-bfa9e16a1407","execution":{"iopub.status.busy":"2022-08-12T08:23:28.424297Z","iopub.execute_input":"2022-08-12T08:23:28.424996Z","iopub.status.idle":"2022-08-12T08:24:44.946888Z","shell.execute_reply.started":"2022-08-12T08:23:28.424965Z","shell.execute_reply":"2022-08-12T08:24:44.945452Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def generate_midi_gpt(length=32):\n    model.eval()\n    input_ids = torch.tensor([TOKEN_START]).unsqueeze(0).long().to('cuda')\n    sample_outputs = model.generate(\n        input_ids,\n        pad_token_id=TOKEN_PAD,\n        eos_token_id=TOKEN_END,\n        min_length=2048,\n        max_length=2048, \n        do_sample=True, \n        top_k=50,\n        top_p=0.95,\n        no_repeat_ngram_size=2,\n        temperature=1,\n        num_return_sequences=1,\n        repition_penalty=5,\n        num_beams=5,\n        )\n    return sample_outputs","metadata":{"id":"_krEgzF2JN69","execution":{"iopub.status.busy":"2022-08-12T08:29:06.263104Z","iopub.execute_input":"2022-08-12T08:29:06.263601Z","iopub.status.idle":"2022-08-12T08:29:06.274229Z","shell.execute_reply.started":"2022-08-12T08:29:06.263558Z","shell.execute_reply":"2022-08-12T08:29:06.272338Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"out = generate_midi_gpt()","metadata":{"id":"enZNOAvUNKGq","execution":{"iopub.status.busy":"2022-08-12T08:29:06.624034Z","iopub.execute_input":"2022-08-12T08:29:06.624391Z","iopub.status.idle":"2022-08-12T08:29:50.651599Z","shell.execute_reply.started":"2022-08-12T08:29:06.624361Z","shell.execute_reply":"2022-08-12T08:29:50.650538Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"midi_out = decode(post_process(out[0][1:].unsqueeze(0), remove_bad_generations=True)[0])\nmidi_out.write('test.mid')","metadata":{"id":"z2kuVF2hNPkh","execution":{"iopub.status.busy":"2022-08-12T08:29:50.653559Z","iopub.execute_input":"2022-08-12T08:29:50.654210Z","iopub.status.idle":"2022-08-12T08:29:50.716580Z","shell.execute_reply.started":"2022-08-12T08:29:50.654161Z","shell.execute_reply":"2022-08-12T08:29:50.715653Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# from music21 import converter,instrument\n\n# s = converter.parse('./test.mid')\n\n# for el in s.recurse():\n#     if 'Instrument' in el.classes: # or 'Piano'\n#         el.activeSite.replace(el, instrument.Contrabassoon())\n\n# s.write('midi', './test_Contrabassoon.mid')","metadata":{},"execution_count":null,"outputs":[]}]}